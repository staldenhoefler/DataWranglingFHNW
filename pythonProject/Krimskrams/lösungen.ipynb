{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.chdir('..')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T09:18:59.776010Z",
     "start_time": "2024-06-24T09:18:59.767044100Z"
    }
   },
   "id": "cf7ee8e50bf49f87",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open('Recognition.txt') as file:\n",
    "    file = file.read()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T09:18:59.792009200Z",
     "start_time": "2024-06-24T09:18:59.778014400Z"
    }
   },
   "id": "dcaa9918130ba70",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'# Sensor based Activity Recognition\\nBeteiligte Personen:\\n- Julia Lobaton\\n- Lukas Zemp\\n- Denis Schatzmann\\n\\nBegleitet durch den Fachexperten:\\n- Marcel Messerli\\n\\n## Datenpipeline\\n### Datenbeschaffung\\nDie Daten wurden von uns mit der App [SensorLogger](https://www.tszheichoi.com/sensorlogger) aufgezeichnet.\\nDabei haben wir uns bei den Trainingsdaten auf die AktivitÃ¤ten Lift fahren, Treppen steigen, gehen, sitzen, stehen, an den StÃ¶cken laufen und Fussball spielen konzentriert.\\nWir haben die Aufnahmen mit 100 Hertz aufgezeichnet. \\nDabei hatten zwei Personen ein IOS-GerÃ¤t und eine Person ein Android-GerÃ¤t.\\nDie Aufnahmen wurden mit den Labels benannt und als .zip-Files exportiert.\\n\\n### Datenimport\\nDer Datenimport wurde [hier](src/dataImporter.py) implementiert. \\n#### Neue Trainingsdaten hinzufÃ¼gen\\n1. Daten mit der App SensorLogger aufzeichnen (100 Hertz, Aktive Sensoren: Accelerometer, Gyroscope, Orientation, Magnetometer, Barometer).\\n2. Stelle sicher, dass alle Aufnahmen richtig benannt sind (NameDerPerson_AktivitÃ¤t-YYYY_MM_DD-HH_MM_SS.zip).\\n3. Lade die .zip-Files in den Ordner [data](data).\\n4. FÃ¼hre den [Datenimporter](src/dataImporter.py) aus.\\n### Resampling\\nDas Resampling wurde [hier](src/resampling.py) implementiert.\\n#### Funktion `resample_data`\\nDie Funktion `resample_data` dient zur Resampling und Vorverarbeitung von CSV-Daten in einem gegebenen Verzeichnis. Ihr Hauptzweck ist es, mehrere CSV-Dateien zu laden, zu bereinigen, auf eine gemeinsame Zeitbasis zu bringen und die Daten neu abzustimmen.\\n1. **Laden der Parameter:**\\n   - Parameter werden aus der YAML-Datei [params](params.yaml) geladen, einschliesslich der Frequenz des Resamplings und der zu wÃ¤hlenden Spalten.\\n\\n2. **Metadaten Extraktion:**\\n   - Extrahiert Metadaten wie Person, Zeit, AktivitÃ¤t und Datum aus dem Ordnernamen.\\n\\n3. **Zeitraum Festlegung:**\\n   - Ermittelt den gemeinsamen Zeitraum fÃ¼r alle CSV-Dateien, indem der frÃ¼heste Startzeitpunkt und der spÃ¤teste Endzeitpunkt bestimmt werden.\\n\\n4. **Datenverarbeitung und Resampling:**\\n   - **Einlesen und Vorbereiten der Daten:**\\n     - Jede CSV-Datei wird eingelesen, die Zeitspalte in ein `datetime`-Format konvertiert und als Index gesetzt. Die Spalte `seconds_elapsed` wird entfernt.\\n\\n   - **Anpassen an den gemeinsamen Zeitraum:**\\n     - Neue Zeilen mit NaN-Werten werden eingefÃ¼gt, um die Datenrahmen an den gemeinsamen Start- und Endzeitpunkt anzupassen.\\n\\n   - **Resampling der Daten:**\\n     - Die Daten werden auf die festgelegte Frequenz resampled, wobei der Medianwert in jedem Intervall berechnet wird. Fehlende Werte werden durch forward-fill ergÃ¤nzt.\\n\\n   - **Umbenennen der Spalten:**\\n     - Die Spaltennamen der resampleten Daten werden umbenannt, um die Herkunft der Daten aus der jeweiligen Datei zu kennzeichnen.\\n\\n5. **Kombination der Daten:**\\n   - Alle resampleten Datenrahmen werden zu einem einzigen DataFrame zusammengefÃ¼hrt.\\n   - Falls die Daten nicht fÃ¼r EDA (Exploratory Data Analysis) gedacht sind, werden nur ausgewÃ¤hlte Spalten beibehalten.\\n   - Es wird ein Train-Validation-Test-Split durchgefÃ¼hrt. Dabei wird auf die Klassenbalance geachtet.\\n\\n6. **RÃ¼ckgabe:**\\n   - Der RÃ¼ckgabewert der Funktion `create_dataset` sind drei custom Dataset-Objekte der Klasse [CustomDataset](src/csvDataset.py), die die resampleten Daten enthalten.\\n\\n#### Verwendung\\n ```python\\n   from src.resampling import create_datasets\\n    train_dataset, val_dataset, test_dataset = create_datasets(folderpath, is_Eda_dataset, echo, train_val_test_split=train_val_test_split)\\n ```\\n\\n### Dataset\\nDie `CustomCsvDataset` Klasse liest und verarbeitet CSV-Dateien, um sie fÃ¼r PyTorch nutzbar zu machen. Sie erbt von der PyTorch `Dataset` Klasse und enthÃ¤lt Funktionen zur Vorverarbeitung und Transformation von Daten.\\n\\n#### Parameter:\\n- **data (Liste von Dictionaries)**: Die Daten aus der CSV-Datei.\\n- **window_size (int)**: Die Anzahl der Zeilen, die als eine Probe betrachtet werden.\\n- **window_overlapping (float)**: Der Anteil der Ãœberlappung zwischen den Fenstern.\\n- **is_EDA_dataset (bool)**: Ein Flag, das angibt, ob der Datensatz fÃ¼r die Explorative Datenanalyse ist.\\n- **transform (bool)**: Ein boolescher Wert, um Transformationen auf die Daten anzuwenden (Standard ist `False`).\\n\\n#### Methoden:\\n- **__init__**: Initialisiert den Datensatz mit den angegebenen Parametern und verarbeitet die Daten bei Bedarf vor.\\n- **__len__**: Gibt die LÃ¤nge des Datensatzes zurÃ¼ck.\\n- **__getitem__**: Ruft eine Probe und ihr Label aus dem Datensatz ab und wendet One-Hot-Encoding auf das Label an.\\n- **preprocess_data**: Verarbeitet die Daten vor, indem Fenster einer bestimmten GrÃ¶sse und Ãœberlappung erstellt und unerwÃ¼nschte Spalten entfernt werden.\\n\\n#### Verwendung:\\n```python\\nfrom src.csvDataset import CustomCsvDataset\\ndataset = CustomCsvDataset(data, window_size, window_overlapping, is_EDA_dataset, transform)\\n```\\nWobei data eine Liste von Dictionaries ist, die die Daten aus der CSV-Datei enthalten. Das dictionary hat folgende Struktur: \\n```python\\noutput_dict = {\\n    \\'data\\': data, \\n    \\'label\\': label, \\n    \\'person\\': person, \\n    \\'activity\\': activity, \\n    \\'date\\': date\\n}\\n```\\n## Explorative Datenanalyse\\n[EDA-Notebook](eda/eda.ipynb)\\n### Daten\\nWir haben zwei getrennte DatensÃ¤tze. Einen fÃ¼rs Trainieren und eines fÃ¼rs Testen.\\nDie Trainingsdaten bestehen aus 189 Aufnahmen von 5 verschiedenen Personen.\\nDas Testset besteht aus 17 Aufnahmen von 3 verschiedenen Personen.\\nDabei ist wichtig zu erwÃ¤hnen, dass die Personen im Testset nicht im Trainingsset vorkommen.\\nDies ermÃ¶glicht es uns, die GeneralisierungsfÃ¤higkeit des Modells zu testen.\\n\\n### Verteilung\\nIn den Trainingsdaten haben wir zwei untervertretene Klassen. Diese sind \"lift\" und \"tschuti\".\\nLift hat etwa 6 % und Tschuti etwa 7 % der Daten.\\nDie Folgen davon sind gut an der [Confusion Matrix](model/random_forest_model.ipynb) zu erkennen. \\nDie beiden Klassen werden deutlich weniger gut erkannt als die restlichen Klassen.\\n\\n### Sensoren\\nDie Sensoren sind grÃ¶sstenteils vom Handy vorgegeben.\\nDie wichtigsten Sensoren sind:\\n- Accelerometer\\n- Gyroscope\\n- Orientation\\n\\nDaten welche wir nicht verwenden sind:\\n- Location\\n\\nDies aus dem Grund, dass das Modell sonst anhand des Ortes erkennen kÃ¶nnte, was gemacht wird.\\n\\n## Parameter in params.yaml file  \\n\\n### resample_freq  \\nMit diesem Parameter kÃ¶nnen wir angeben, auf wie viele Frequenzen wir die Daten resamplen. \\n\\n### window_size  \\nDas ist die GrÃ¶sse des Datenpunktes, welches das Modell als Input erhÃ¤lt.  \\nwindow_size : resample_freq = Sekunden\\n\\n### cutting_size  \\nMit diesem Parameter kann angegeben werden, wie viele Sekunden am Anfang und am Ende der Aufnahme abgeschnitten werden sollen.   \\ncutting_size : resample_freq = Sekunden\\n\\n### window_overlap  \\nDieser Parameter gibt an, um wie viel Prozent sich die Fenster (Datenpunkte) Ã¼berlappen.  \\nEin Wert von 0.5 bedeutet, dass sich die Fenster um 50% Ã¼berlappen. \\n\\n### max_splitting_len  \\nDa die Aufnahmen ganz unterschiedlich lang sind, haben wir uns entschieden, die Aufnahmen zu splitten.  \\nMit diesem Parameter kÃ¶nnen wir angeben, wie lange die Aufnahmen maximal sein dÃ¼rfen.  \\nmax_splitting_len : resample_freq = Sekunden\\n\\n### selected_columns  \\nDiese Features werden fÃ¼r die Modelle verwendet. Wir haben jegliche Features gelÃ¶scht, welche auf den Ort hinweisen. Diese kÃ¶nnten sonst ein Indikator fÃ¼r die Person sein. Wenn eine Person immer am gleichen Ort die gleiche AktivitÃ¤t ausfÃ¼hrt, kÃ¶nnte das Modell das anhand des Ortes erkennen.\\n\\n## Deep Learning Modell oder Machine Learning Modell? Was ist fÃ¼r dieses Projekt besser geeignet?\\n## Random Forest\\n\\n#### Datenvorbereitung\\nDas Random Forest Modell kann nur mit zwei Dimensionen umgehen. Deshalb mÃ¼ssen wir die Daten so umformen, dass sie nur noch zwei Dimensionen haben.\\nDies erreichen wir in dem man die Hertz-Aufnahmen hintereinander gestacket werden.\\n\\n#### Feature Engineering\\nDas Random Forest Modell versteht das Konzept von Zeitreihen nicht. Deshalb werden neue Features erzeugt.\\nDiese sind:\\n1. **Mean (Durchschnitt)**: Der Mittelwert der Werte in der Spalte.\\n2. **Standard Deviation (Standardabweichung)**: Die Standardabweichung der Werte in der Spalte.\\n3. **Min (Minimum)**: Der kleinste Wert in der Spalte.\\n4. **Max (Maximum)**: Der grÃ¶ÃŸte Wert in der Spalte.\\n5. **Median**: Der Median der Werte in der Spalte.\\n6. **25th Percentile**: Der Wert des 25. Perzentils der Werte in der Spalte.\\n7. **75th Percentile**: Der Wert des 75. Perzentils der Werte in der Spalte.\\n8. **Range (Spannweite)**: Der Unterschied zwischen dem grÃ¶ÃŸten und dem kleinsten Wert in der Spalte.\\n9. **Sum (Summe)**: Die Summe aller Werte in der Spalte.\\n10. **Variance (Varianz)**: Die Varianz der Werte in der Spalte.\\n11. **Root Mean Square (Quadratischer Mittelwert)**: Die Quadratwurzel des Mittelwerts der quadrierten Werte in der Spalte.\\n12. **Slope (Steigung)**: Die Steigung der linearen Regression der Werte in der Spalte.\\n13. **Intercept (Y-Achsenabschnitt)**: Der Y-Achsenabschnitt der linearen Regression der Werte in der Spalte.\\n14. **R-Value**: Der Korrelationskoeffizient der linearen Regression der Werte in der Spalte.\\n15. **P-Value**: Der P-Wert der linearen Regression der Werte in der Spalte.\\n16. **Standard Error (Standardfehler)**: Der Standardfehler der linearen Regression der Werte in der Spalte.\\n\\nDurch das HinzufÃ¼gen dieser Features ist das Modell merklich besser geworden.\\n\\n#### Hyperparameter Tuning\\nUm die besten Hyperparameter zu finden haben wir ein GridSearchCV durchgefÃ¼hrt. \\nJedoch overfitten die Daten massiv, wodurch GridSearch nicht mehr sinnvoll war.\\nDeswegen wuden die Hyperparameter manuell eingestellt.\\n\\n#### Ergebnisse\\nDas Ergebnis fÃ¤llt erstaunlich gut aus. \\nDas Modell erreicht auf das Custom-Testset einen F1-Score von 40%.\\n\\n## MLP  \\n\\n## CNN Modell\\nBeim CNN Modell haben wir viele der Parameter einzeln getestet und optimiert.  \\nSomit konnten wir sehen, was die einzelnen Parameter fÃ¼r einen Einfluss auf das Modell haben.    \\nSchlussendlich waren die besten Ergebnisse folgende:  \\n- Validation Loss: 0.188  \\n- Validation F1 Score: 0.928  \\n- Validation Accuracy: 0.9414  \\n\\nWelche Parameter dafÃ¼r verwendet werden, kann im Notebook `cnn_modell.ipynb` nachgeschaut werden.  \\nAuf neuen Daten (neue Personen) schnitt das CNN Modell schlechter ab, als der Random Forest.\\n\\n## RNN  \\nAls zweites Deep Learning Modell haben wir uns fÃ¼r ein RNN Modell entschieden. Dieses Modell ist in der Lage, die zeitliche AbhÃ¤ngigkeit der Daten zu lernen. Es ist also gut geeignet fÃ¼r Daten mit einer zeitlichen Abfolge.  \\nDas Training, Evaluieren und Testen des Modells ist im Notebook `rnn_modell.ipynb` dokumentiert.  \\n\\n#### Testen auf wenigen Daten\\nAuf wenigen Daten konnte das RNN die Daten sehr schnell auswendig lernen. Somit wussten wir, dass das Modell in der Lage ist die Daten zu lernen. \\n\\n#### Testen auf ganzen Daten  \\nBeim RNN Modell konnten wir leider aus zeitlichen GrÃ¼nden nicht ein so intensives Hyperparameter Tuning durchfÃ¼hren wie bei den anderen Modellen.  \\nWir testeten manuell ein paar Kombinationen.  \\nDie besten Ergebnisse welche wir erzielen konnten waren folgende:  \\n- Validation Loss: 0.3817  \\n- Validation F1 Score: 0.8939\\n- Validation Accuracy: 0.9148  \\n\\nHier ist wichtig zu erwÃ¤hnen, dass wir nicht mehr viel Zeit hatten, ein intensives Hyperparameter Tuning durchzufÃ¼hren.  \\nWir sind Ã¼berzeugt, dass das Modell noch besser werden kÃ¶nnte, wenn wir mehr Zeit hÃ¤tten.\\n\\n#### Testen auf neuen Daten (neue Personen)  \\nHier schnitt das Modell schlechter als das random Forest Modell ab.\\n\\n## MLP  \\nAls drittes Deeplearning Modell haben wir uns entschieden noch ein Multilayerperceptron zu erstellen. \\nDas MLP wurde mittels verschiedenen Anzahlen an Hiddenlayer erstellt. \\n\\n#### Testen auf wenigen Daten\\nAuf wenigen Daten konnte auch das MLP die Daten sehr schnell auswendig lernen. Somit wussten wir, dass das Modell in der Lage ist die Daten zu lernen. \\n\\n#### Testen auf ganzen Daten  \\nBeim MLP Modell konnten wir leider aus zeitlichen GrÃ¼nden ebenfalls nicht ein so intensives Hyperparameter Tuning durchfÃ¼hren wie bei den anderen Modellen. Auch war es uns nicht mehr mÃ¶glich die K-Fold Crossvalidation korrekt umzusetzen. \\n\\nDie besten Ergebnisse welche wir erzielen konnten waren folgende:  \\n- Validation Loss: 0.1087  \\n- Validation F1 Score: 0.9615\\n- Validation Accuracy: 0.9663  \\n\\n#### Testen auf neuen Daten (neue Personen)  \\nAus den genannten zeitlichen GrÃ¼nden, konnten wir die K-Fold Crossvalidation im MLP nicht korrekt implementieren. Daher konnten wir das beste MLP auch nicht auf die neuen (ungesehenen) Testdaten anwenden.\\n\\n### Verwendetes Modell  \\nSchlussendlich haben wir uns dann fÃ¼r das Random Forest Modell entschieden. Dies aus dem Grund, da das Random Modell am besten auf das Testset mit den neuen (ungesehenen) Daten performt hat. Daher macht es Sinn, dass wir dieses Modell fÃ¼r das Dashboard verwenden, weil wir dort nur neue und ungesehene Daten erhalten. \\n\\n## Dashboard\\nWir haben ein interaktives Dashboard mit Plotly Dash erstellt. Um das Dashboard aufzurufen muss der code: python -m dashboard.app in der Python Konsole eingegeben werden. Dann kann man auf den localhost zugreifen und kommt auf die Startseite des Dashboards, wo man einen Button \"Please select a Zip File\" findet. Dort kann man ein Zip File von den Sensordaten hochladen und die entsprechenden AktivitÃ¤ten werden angezeigt. Im calculations.py File werden die Berechnungen fÃ¼r die verbrannten Kallorien gemacht, welche dann ebenfalls im Dashboard angezeigt werden. Auch wird einem ein Bild angezeigt, je nach dem welche AktivitÃ¤t man am hÃ¤ufigsten, respektive am lÃ¤ngsten gemacht hat. \\n\\n## Weitere mÃ¶gliche Schritte  \\nWenn wir noch mehr Zeit hÃ¤tten, kÃ¶nnten wir noch folgende Schritte unternehmen:  \\n- Mehr Daten sammeln von vielen verschiedenen Personen  \\n- Eine Idee wie man viele Daten sammeln kÃ¶nnte wÃ¤re, dass wenn jemand ein File hochlÃ¤dt, dass diese dann auch dem Modell zur VerfÃ¼gung gestellt werden. Der Nutzer kÃ¶nnte dann abgeben, welches Label das File hat.\\n\\n## Fragestellung\\nDie Fragestellung, respektive das Ziel wurde am Anfang der Challenge in dem Konzept definiert, welches wir an Marcel Messerli gesendet haben. \\n\\nDie Probleme: Das Zeil der ModellgÃ¼te haben wir auf eine Accuracy von 90% festgelegt. Die Metrik Accuracy macht aber nicht viel Sinn, wie wir im Laufe der Challenge herausgefunden haben. Da wir einen unausgewogenen Datensatz haben, sprich nicht jedes Label gleichhÃ¤ufig vorkommt, kann eine Accuracy nicht wirklich viel Ã¼ber die GÃ¼te der Modelle sagen. Beispielsweise kÃ¶nnte man, wenn 90% der Daten das Label \"Laufen\" hÃ¤tten, einfach ein Modell erstellen, welches immer \"Laufen\" vorhersagt und wÃ¼rde dann eine Accuracy von 90% erhalten. Um dem entgegen zu wirken, haben wir beim Modell Training und der Evaluierung den Macro F1-Score verwendet, welcher eine Classinbalance berÃ¼cksichtigt. Auch haben wir uns vorallem darauf geachtet, wo der Loss in den Validierungsdaten am tiefsten war. \\n\\nEbenfalls haben wir uns als Ziel gesetzt, ein Dashboard zu erstellen, wo man eine Zip-Datei mit den Sensordaten hochladen kann. Dann sollte die Datei von unserem Modell analysiert werden und es sollte angezeigt werden, welche AktivitÃ¤t ausgeÃ¼bt wurde und wieviele Kalorien die Person dabei verbrannt hat. Dies ist uns gut gelungen.\\n\\n\\n\\n\\n'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T09:18:59.807008500Z",
     "start_time": "2024-06-24T09:18:59.795011500Z"
    }
   },
   "id": "fda50292ee896bb4",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                        name     location  \\\n0  University of Switzerland  Switzerland   \n\n                                         departments  \n0  [{'name': 'Computer Science', 'courses': [{'co...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>location</th>\n      <th>departments</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>University of Switzerland</td>\n      <td>Switzerland</td>\n      <td>[{'name': 'Computer Science', 'courses': [{'co...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('jsonBeispiel.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "pd.DataFrame([data['university']])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T09:38:08.717884900Z",
     "start_time": "2024-06-24T09:38:08.693882500Z"
    }
   },
   "id": "dc32646ff9ae60ee",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
